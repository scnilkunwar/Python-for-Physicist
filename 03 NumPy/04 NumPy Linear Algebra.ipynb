{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python for Physicist](https://github.com/scnilkunwar/Python-for-Physicist/blob/main/images/Banner.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    \n",
    "![Numpy](https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg)\n",
    "  <h1> Python for Physicist - NumPy Linear Algebra</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://numpy.org/doc/stable/reference/routines.linalg.html\" \n",
    "style=\"display: inline-block; \n",
    "padding: 10px 20px; \n",
    "background-color: #4CAF50; \n",
    "color: white; \n",
    "text-align: center; \n",
    "text-decoration: none; \n",
    "border-radius: 5px;\">NumPy linalg</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Functions in NumPy\n",
    "NumPy provides a wide array of functions for linear algebra, ranging from basic matrix operations to more advanced factorizations and decompositions. Below is a categorized list of key linear algebra functions in NumPy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Linear Algebra Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication\n",
    "\n",
    "`np.dot()`\n",
    "\n",
    "Description: Computes the dot product of two arrays or matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Dot product\n",
    "a = np. array ([[1 , 2], [3, 4]])\n",
    "b = np. array ([[5 , 6], [7, 8]])\n",
    "dot_product = np. dot(a, b) # can be done using @ operator\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.matmul()`\n",
    "\n",
    "Description: Performs matrix multiplication, equivalent to the dot product for 2-D\n",
    "arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_product = np. matmul (a, b)\n",
    "mat_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.vdot()`\n",
    "\n",
    "Description: Computes the dot product of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "vdot_product = np. vdot (a[0] , b [0])\n",
    "print(vdot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.cross()` \n",
    "\n",
    "The np.cross function in NumPy computes the cross product of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3  6 -3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Compute the cross product\n",
    "cross_product = np.cross(a, b)\n",
    "\n",
    "print(cross_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose\n",
    "`np.transpose()`\n",
    "\n",
    "Description: Transposes the given array, flipping it over its diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np. array ([[1 , 2], [3, 4]])\n",
    "print(a)\n",
    "transposed_a = np.transpose(a) #a.T\n",
    "print(transposed_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.swapaxes()`\n",
    "\n",
    "Description: Swaps two axes of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swapped_axes = np.swapaxes (a, 0, 1)\n",
    "swapped_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace\n",
    "`np.trace`\n",
    "Description: Computes the sum of the diagonal elements of a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trace of an array\n",
    "trace_a = np.trace(a)\n",
    "trace_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Linear Systems\n",
    "#### Solve Ax = b\n",
    "`np.linalg.solve(a, b)`\n",
    "\n",
    "Description: Solves the linear equation Ax = b for x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the systemm of linear equation:\n",
    "\n",
    "$$\n",
    "3x + 2y = 7 \\\\\n",
    "x - 3y = -5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3, 2],\n",
    "              [1, -3]])\n",
    "b = np. array ([7 , -5])\n",
    "x = np.linalg.solve (a, b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Inversion\n",
    "`np.linalg.inv(a)`\n",
    "Description: Computes the inverse of a square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27272727,  0.18181818],\n",
       "       [ 0.09090909, -0.27272727]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the inverse of a matrix\n",
    "inv_a = np.linalg.inv(a)\n",
    "inv_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving using inverse of matrix.\n",
    "\n",
    "Solve the systemm of linear equation:\n",
    "\n",
    "$$\n",
    "2x + y = -1 \\\\\n",
    "5x - 3y = -8\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1.]\n",
      "[-1.  1.]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2, 1],\n",
    "              [5, -3]])\n",
    "b = np. array ([-1 , -8])\n",
    "inv_a = np.linalg.inv(a)\n",
    "x = inv_a @ b\n",
    "print(x)\n",
    "print(np.linalg.solve(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1.,  1.]), array([], dtype=float64), np.int32(2), array([5.96667765, 1.84357203]))\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.lstsq(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompositions\n",
    "## Eigenvalue Decomposition\n",
    "`np.linalg.eig(a)`\n",
    "\n",
    "Description: Computes the eigenvalues and right eigenvectors of a square array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.85410197 -3.85410197]\n",
      "[[ 0.76039797 -0.16838141]\n",
      " [ 0.6494574   0.98572192]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalue and eigenvector computation\n",
    "eigenvalues , eigenvectors = np. linalg .eig(a)\n",
    "print(eigenvalues)\n",
    "print(eigenvectors) #each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  1]\n",
      " [ 5 -3]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64895911 -3.79907279]\n"
     ]
    }
   ],
   "source": [
    "print(a @ eigenvectors[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64895911 -3.79907279]\n"
     ]
    }
   ],
   "source": [
    "print(eigenvalues[1] * eigenvectors[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "`np.linalg.svd()`\n",
    "\n",
    "Description: Factorizes a matrix into three matrices, representing its intrinsic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "**Singular Value Decomposition (SVD)** is a fundamental concept in linear algebra, particularly useful in various applications such as signal processing, statistics, and machine learning. It decomposes a matrix into three other matrices, revealing important properties of the original matrix.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For any $( m \\times n )$ matrix $ A$, SVD states that you can decompose $ A $ into the following form:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ U $ is an $( m \\times m )$ orthogonal matrix whose columns are the left singular vectors of \\( A \\).\n",
    "- $( \\Sigma )$ is an $( m \\times n )$ diagonal matrix with non-negative real numbers on the diagonal, known as the singular values of $ A $.\n",
    "- $( V^T )$ is the transpose of an $( n \\times n )$ orthogonal matrix $ V $, whose columns are the right singular vectors of $ A $.\n",
    "\n",
    "### Properties\n",
    "1. **Orthogonality**: The columns of \\( U \\) and \\( V \\) are orthonormal.\n",
    "   - $( U^T U = I )$\n",
    "   - $( V^T V = I )$\n",
    "2. **Singular Values**: The singular values in $ \\Sigma 0$ are sorted in descending order:\n",
    "   $$\n",
    "   \\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r \\geq 0\n",
    "   $$\n",
    "   where \\( r \\) is the rank of the matrix \\( A \\).\n",
    "3. **Reconstruction**: The original matrix \\( A \\) can be reconstructed from \\( U \\), $( \\Sigma )$, and $( V^T )$.\n",
    "\n",
    "### Applications\n",
    "- **Dimensionality Reduction**: SVD is widely used in Principal Component Analysis (PCA) for reducing the number of features in data.\n",
    "- **Image Compression**: It helps in approximating images with fewer components, thereby reducing storage requirements.\n",
    "- **Recommender Systems**: SVD is utilized in collaborative filtering for recommending items to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.2229892 , -0.97482092],\n",
       "        [-0.97482092,  0.2229892 ]]),\n",
       " array([5.96667765, 1.84357203]),\n",
       " array([[-0.89163238,  0.4527601 ],\n",
       "        [-0.4527601 , -0.89163238]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singular Value Decomposition\n",
    "u, s, vh = np.linalg.svd(a)\n",
    "u, s, vh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "`np.linalg.cholesky(a)`\n",
    "Description: Decomposes a positive-definite matrix into a lower triangular matrix and\n",
    "its transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "\n",
    "**Cholesky decomposition** is a method of decomposing a symmetric, positive-definite matrix into the product of a lower triangular matrix and its transpose. It is commonly used in numerical analysis, particularly for solving systems of linear equations and performing matrix inversion.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For a symmetric positive-definite matrix $ A $, the Cholesky decomposition states:\n",
    "\n",
    "$$\n",
    "A = L L^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ A $ is the original matrix.\n",
    "- $ L $ is a lower triangular matrix.\n",
    "- $ L^T $ is the transpose of $ L $.\n",
    "\n",
    "### Properties\n",
    "1. **Symmetric**: The original matrix $ A $ must be symmetric, meaning $ A = A^T $.\n",
    "2. **Positive-Definite**: The matrix must be positive-definite, which generally implies that all its leading principal minors are positive.\n",
    "3. **Unique**: The decomposition is unique for symmetric positive-definite matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix A:\n",
      "[[4 2]\n",
      " [2 3]]\n",
      "\n",
      "Lower Triangular Matrix L:\n",
      "[[2.         0.        ]\n",
      " [1.         1.41421356]]\n",
      "\n",
      "Reconstructed Matrix A from L:\n",
      "[[4. 2.]\n",
      " [2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Define a symmetric positive-definite matrix\n",
    "A = np.array([[4, 2],\n",
    "              [2, 3]])\n",
    "\n",
    "# Perform Cholesky decomposition\n",
    "L = np.linalg.cholesky(A)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nLower Triangular Matrix L:\")\n",
    "print(L)\n",
    "\n",
    "# Verify that A = L @ L^T\n",
    "A_reconstructed = L @ L.T\n",
    "print(\"\\nReconstructed Matrix A from L:\")\n",
    "print(A_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "`np.linalg.qr(a)`\n",
    "\n",
    "Description: Decomposes a matrix into an orthogonal matrix and an upper triangular\n",
    "matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "**QR decomposition** is a method of decomposing a matrix into a product of an orthogonal matrix and an upper triangular matrix. This decomposition is widely used in numerical linear algebra, particularly for solving linear systems, least squares problems, and eigenvalue computations.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For a given matrix $ A $ of size $ m \\times n $, the QR decomposition states:\n",
    "\n",
    "$$\n",
    "A = Q R\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ A $ is the original matrix.\n",
    "- $ Q $ is an orthogonal matrix (or orthonormal if its columns are normalized).\n",
    "- $ R $ is an upper triangular matrix.\n",
    "\n",
    "### Properties\n",
    "1. **Orthogonality**: The columns of the matrix $ Q $ are orthogonal (i.e., $ Q^T Q = I $), where $ I $ is the identity matrix.\n",
    "2. **Upper Triangular**: The matrix $ R $ is upper triangular, meaning all entries below the main diagonal are zero.\n",
    "3. **Unique**: The QR decomposition is unique when the columns of $ A $ are linearly independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix A:\n",
      "[[ 12 -51   4]\n",
      " [  6 167 -68]\n",
      " [ -4  24 -41]]\n",
      "\n",
      "Orthogonal Matrix Q:\n",
      "[[-0.85714286  0.39428571  0.33142857]\n",
      " [-0.42857143 -0.90285714 -0.03428571]\n",
      " [ 0.28571429 -0.17142857  0.94285714]]\n",
      "\n",
      "Upper Triangular Matrix R:\n",
      "[[ -14.  -21.   14.]\n",
      " [   0. -175.   70.]\n",
      " [   0.    0.  -35.]]\n",
      "\n",
      "Reconstructed Matrix A from Q and R:\n",
      "[[ 12. -51.   4.]\n",
      " [  6. 167. -68.]\n",
      " [ -4.  24. -41.]]\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix A\n",
    "A = np.array([[12, -51, 4],\n",
    "              [6, 167, -68],\n",
    "              [-4, 24, -41]])\n",
    "\n",
    "# Perform QR decomposition\n",
    "Q, R = np.linalg.qr(A)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nOrthogonal Matrix Q:\")\n",
    "print(Q)\n",
    "print(\"\\nUpper Triangular Matrix R:\")\n",
    "print(R)\n",
    "\n",
    "# Verify that A = Q @ R\n",
    "A_reconstructed = Q @ R\n",
    "print(\"\\nReconstructed Matrix A from Q and R:\")\n",
    "print(A_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms and Determinants\n",
    "### Norms\n",
    "`np.linalg.norm(a, ord = None )`\n",
    "\n",
    "Description: Computes the norm (length) of a vector or the Frobenius norm of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of a Vector\n",
    "\n",
    "The **norm** of a vector is a measure of its length or magnitude. It provides a way to quantify the size of a vector in a vector space.\n",
    "\n",
    "1. **Euclidean Norm** ($L^2$ Norm):\n",
    "   The Euclidean norm of a vector $ \\mathbf{v} \\in \\mathbb{R}^n $ is defined as:\n",
    "\n",
    "   $$\n",
    "   \\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}\n",
    "   $$\n",
    "\n",
    "   where $ v_i $ are the components of the vector $ \\mathbf{v} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector v:\n",
      "[3 4]\n",
      "\n",
      "Euclidean Norm (L2): 5.0\n"
     ]
    }
   ],
   "source": [
    "# Define a vector\n",
    "v = np.array([3, 4])\n",
    "\n",
    "euclidean_norm = np.linalg.norm(v)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"Vector v:\")\n",
    "print(v)\n",
    "print(\"\\nEuclidean Norm (L2):\", euclidean_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of a Matrix\n",
    "\n",
    "The **norm** of a matrix is a measure of the size or magnitude of the matrix. Similar to vector norms, matrix norms provide insights into the behavior of linear transformations represented by matrices.\n",
    "\n",
    "1. **Frobenius Norm** ($\\|\\cdot\\|_F$):\n",
    "   The Frobenius norm of a matrix $ A \\in \\mathbb{R}^{m \\times n} $ is defined as:\n",
    "\n",
    "   $$\n",
    "   \\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}\n",
    "   $$\n",
    "\n",
    "   where $ a_{ij} $ are the elements of the matrix $ A $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Frobenius Norm: 16.881943016134134\n",
      "Infinity Norm: 24.0\n",
      "1-Norm: 18.0\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix A\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Calculate Frobenius norm\n",
    "frobenius_norm = np.linalg.norm(A)\n",
    "\n",
    "# Calculate Infinity norm\n",
    "infinity_norm = np.linalg.norm(A, np.inf)\n",
    "\n",
    "# Calculate 1-norm\n",
    "one_norm = np.linalg.norm(A, 1)\n",
    "\n",
    "# Display the results\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nFrobenius Norm:\", frobenius_norm)\n",
    "print(\"Infinity Norm:\", infinity_norm)\n",
    "print(\"1-Norm:\", one_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant\n",
    "`np.linalg.det(a)`\n",
    "\n",
    "Description: Computes the determinant of a square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.0\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix A\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 6, 6],\n",
    "              [7, 8, 9]])\n",
    "det_a = np.linalg.det(A)\n",
    "print(det_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Rank\n",
    "`np.linalg.matrix_rank(a)`\n",
    "\n",
    "Description: Returns the rank of a matrix, which is the dimension of the vector space\n",
    "generated by its rows or columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of a Matrix\n",
    "\n",
    "The **rank** of a matrix is a fundamental concept in linear algebra that provides important information about the matrix's properties. Specifically, the rank indicates the maximum number of linearly independent row or column vectors in the matrix. It helps determine the dimensions of the row space and column space of the matrix.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For a matrix $ A \\in \\mathbb{R}^{m \\times n} $, the rank is defined as:\n",
    "\n",
    "$$\n",
    "\\text{rank}(A) = \\text{dim}(\\text{Row space of } A) = \\text{dim}(\\text{Column space of } A)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\text{dim}(\\cdot)$ denotes the dimension of a vector space.\n",
    "- The row space consists of all linear combinations of the row vectors of $ A $.\n",
    "- The column space consists of all linear combinations of the column vectors of $ A $.\n",
    "\n",
    "### Properties\n",
    "1. **Rank Range**: The rank of a matrix $ A $ satisfies the inequality:\n",
    "   $$\n",
    "   0 \\leq \\text{rank}(A) \\leq \\min(m, n)\n",
    "   $$\n",
    "   where $ m $ is the number of rows and $ n $ is the number of columns.\n",
    "\n",
    "2. **Full Rank**: A matrix is said to have full rank if $\\text{rank}(A) = \\min(m, n)$.\n",
    "\n",
    "3. **Linear Dependence**: If the rank of a matrix is less than its number of rows or columns, it indicates that there are linearly dependent vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Rank of Matrix A: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix A\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Calculate the rank of the matrix\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "\n",
    "# Display the result\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nRank of Matrix A:\", rank_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix A\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 6, 6],\n",
    "              [7, 8, 9]])\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Inverse of a Matrix\n",
    "\n",
    "The **pseudo-inverse** of a matrix is a generalization of the matrix inverse that can be applied to non-square or singular matrices. The pseudo-inverse is particularly useful in solving linear equations, especially in least squares problems.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For a matrix $ A \\in \\mathbb{R}^{m \\times n} $, the pseudo-inverse, denoted as $ A^+ $, can be defined using the Singular Value Decomposition (SVD) or by using the Moore-Penrose conditions. The pseudo-inverse satisfies the following conditions:\n",
    "\n",
    "1. $ AA^+A = A $\n",
    "2. $ A^+AA^+ = A^+ $\n",
    "3. $ (AA^+)^T = AA^+ $\n",
    "4. $ (A^+A)^T = A^+A $\n",
    "\n",
    "The pseudo-inverse is computed as:\n",
    "\n",
    "$$\n",
    "A^+ = V \\Sigma^+ U^T\n",
    "$$\n",
    "\n",
    "where $ A = U \\Sigma V^T $ is the SVD of $ A $, and $ \\Sigma^+ $ is obtained by taking the reciprocal of the non-zero singular values in $ \\Sigma $ and transposing the resulting matrix.\n",
    "\n",
    "### Pseudo-Inverse in NumPy\n",
    "\n",
    "You can calculate the pseudo-inverse of a matrix using NumPy with the `numpy.linalg.pinv` function. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Pseudo-Inverse of Matrix A:\n",
      "[[-0.94444444  0.44444444]\n",
      " [-0.11111111  0.11111111]\n",
      " [ 0.72222222 -0.22222222]]\n",
      "\n",
      "Verify AA^+A = A:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "Verify A^+AA^+ = A^+:\n",
      "[[-0.94444444  0.44444444]\n",
      " [-0.11111111  0.11111111]\n",
      " [ 0.72222222 -0.22222222]]\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix A\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# Calculate the pseudo-inverse of the matrix\n",
    "pseudo_inverse_A = np.linalg.pinv(A)\n",
    "\n",
    "# Display the results\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nPseudo-Inverse of Matrix A:\")\n",
    "print(pseudo_inverse_A)\n",
    "\n",
    "# Verify the properties of the pseudo-inverse\n",
    "print(\"\\nVerify AA^+A = A:\")\n",
    "print(np.dot(A, np.dot(pseudo_inverse_A, A)))\n",
    "\n",
    "print(\"\\nVerify A^+AA^+ = A^+:\")\n",
    "print(np.dot(pseudo_inverse_A, np.dot(A, pseudo_inverse_A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Product\n",
    "`np.cross()`\n",
    "\n",
    "Description: Computes the cross product of two 3-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3,  6, -3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the cross product of two 3D vectors\n",
    "a_vec = np. array ([1 , 2, 3])\n",
    "b_vec = np. array ([4 , 5, 6])\n",
    "cross_product = np. cross (a_vec , b_vec )\n",
    "cross_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker Product\n",
    "\n",
    "The **Kronecker product** is a mathematical operation on two matrices that produces a block matrix. It is denoted by the symbol $\\otimes$. The Kronecker product is used in various fields such as mathematics, engineering, and physics, especially in systems involving tensor products.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For two matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{p \\times q}$, the Kronecker product $A \\otimes B$ results in a matrix of size $(m \\cdot p) \\times (n \\cdot q)$. The entries of the Kronecker product are computed as follows:\n",
    "\n",
    "$$\n",
    "A \\otimes B = \n",
    "\\begin{bmatrix}\n",
    "a_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\\n",
    "a_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1}B & a_{m2}B & \\cdots & a_{mn}B\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $a_{ij}$ are the elements of matrix $A$.\n",
    "\n",
    "### Properties\n",
    "\n",
    "1. **Dimensions**: If $A$ is $m \\times n$ and $B$ is $p \\times q$, then $A \\otimes B$ is $(m \\cdot p) \\times (n \\cdot q)$.\n",
    "\n",
    "2. **Associativity**: The Kronecker product is associative, meaning $(A \\otimes B) \\otimes C = A \\otimes (B \\otimes C)$.\n",
    "\n",
    "3. **Distributivity**: The Kronecker product is distributive over matrix addition:\n",
    "   $$\n",
    "   A \\otimes (B + C) = (A \\otimes B) + (A \\otimes C)\n",
    "   $$\n",
    "\n",
    "4. **Identity Matrix**: The Kronecker product with an identity matrix behaves similarly to scalar multiplication:\n",
    "   $$\n",
    "   A \\otimes I_n = A \\text{ (where } I_n \\text{ is the } n \\times n \\text{ identity matrix)}\n",
    "   $$\n",
    "\n",
    "### Applications\n",
    "- **Quantum Mechanics**: Used in representing multi-particle systems.\n",
    "- **Signal Processing**: Helps in the analysis of signals with multiple channels.\n",
    "- **Statistics**: Useful in multivariate statistics and regression analysis.\n",
    "- **Computer Graphics**: Employed in transformations and rendering techniques.\n",
    "### Kronecker Product in NumPy\n",
    "\n",
    "You can compute the Kronecker product in Python using NumPy with the `numpy.kron` function. Here’s how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Matrix B:\n",
      "[[0 5]\n",
      " [6 7]]\n",
      "\n",
      "Kronecker Product A ⊗ B:\n",
      "[[ 0  5  0 10]\n",
      " [ 6  7 12 14]\n",
      " [ 0 15  0 20]\n",
      " [18 21 24 28]]\n"
     ]
    }
   ],
   "source": [
    "# Define matrices A and B\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[0, 5],\n",
    "              [6, 7]])\n",
    "\n",
    "# Calculate the Kronecker product\n",
    "kron_product = np.kron(A, B)\n",
    "\n",
    "# Display the results\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "print(\"\\nKronecker Product A ⊗ B:\")\n",
    "print(kron_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Dot Product\n",
    "\n",
    "The **tensor dot product** is a generalization of the dot product that can be applied to tensors of any rank (dimension). It combines two tensors and contracts over specified axes, resulting in a new tensor. This operation is commonly used in fields such as physics, engineering, and machine learning.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Given two tensors $A$ and $B$, the tensor dot product is defined as:\n",
    "\n",
    "$$\n",
    "C = A \\odot B\n",
    "$$\n",
    "\n",
    "Where $C$ is the resulting tensor after performing the dot product.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "For tensors $A$ and $B$, the tensor dot product is computed by summing the products of their elements over the specified axes. If $A$ is of shape $(i_1, i_2, \\ldots, i_m)$ and $B$ is of shape $(j_1, j_2, \\ldots, j_n)$, the tensor dot product will produce a tensor with dimensions determined by the axes specified for contraction.\n",
    "\n",
    "### Example\n",
    "\n",
    "For instance, if we have:\n",
    "\n",
    "- Tensor $A$ with shape $(2, 3)$\n",
    "- Tensor $B$ with shape $(3, 4)$\n",
    "\n",
    "The tensor dot product can be computed over the second axis of $A$ and the first axis of $B$, resulting in a tensor $C$ of shape $(2, 4)$.\n",
    "\n",
    "### Properties\n",
    "\n",
    "1. **Associativity**: The tensor dot product is associative:\n",
    "   $$\n",
    "   A \\odot (B \\odot C) = (A \\odot B) \\odot C\n",
    "   $$\n",
    "\n",
    "2. **Distributivity**: The tensor dot product is distributive over addition:\n",
    "   $$\n",
    "   A \\odot (B + C) = A \\odot B + A \\odot C\n",
    "   $$\n",
    "\n",
    "3. **Commutativity**: In general, the tensor dot product is not commutative:\n",
    "   $$\n",
    "   A \\odot B \\neq B \\odot A\n",
    "   $$\n",
    "\n",
    "4. **Linearity**: The tensor dot product is linear in each of its arguments. That is, for scalars $\\alpha$ and $\\beta$:\n",
    "   $$\n",
    "   \\alpha (A \\odot B) + \\beta (A \\odot C) = A \\odot (\\alpha B + \\beta C)\n",
    "   $$\n",
    "\n",
    "5. **Inner Product**: For vector spaces, if both tensors are vectors (1D tensors), then the tensor dot product corresponds to the standard inner product.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Physics**: Used in calculations involving vector and tensor fields, such as in continuum mechanics and relativity.\n",
    "\n",
    "- **Machine Learning**: Useful in neural networks, particularly in operations involving tensors for deep learning, where inputs and weights are often represented as multi-dimensional arrays.\n",
    "\n",
    "- **Computer Graphics**: Applied in transformations and rendering algorithms involving multi-dimensional data, especially in 3D graphics and simulations.\n",
    "\n",
    "- **Signal Processing**: Used in multi-channel signal processing, where signals are represented as tensors and various operations are performed.\n",
    "\n",
    "- **Quantum Computing**: In quantum mechanics, the states of systems are often represented as tensors, and tensor dot products are used in operations involving quantum states.\n",
    "\n",
    "### Tensor Dot Product in NumPy\n",
    "\n",
    "In Python, you can compute the tensor dot product using the `numpy.tensordot` function. Here’s how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Tensor B:\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "\n",
      "Tensor Dot Product A • B:\n",
      "[[ 58  64]\n",
      " [139 154]]\n"
     ]
    }
   ],
   "source": [
    "# Define tensors A and B\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "B = np.array([[7, 8],\n",
    "              [9, 10],\n",
    "              [11, 12]])\n",
    "\n",
    "# Calculate the tensor dot product\n",
    "tensor_dot_product = np.tensordot(A, B, axes=1)\n",
    "\n",
    "# Display the results\n",
    "print(\"Tensor A:\")\n",
    "print(A)\n",
    "print(\"\\nTensor B:\")\n",
    "print(B)\n",
    "print(\"\\nTensor Dot Product A • B:\")\n",
    "print(tensor_dot_product)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
